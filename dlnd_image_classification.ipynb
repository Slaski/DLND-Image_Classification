{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CIFAR-10 Dataset: 171MB [30:59, 91.7KB/s]                                                                                                                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 3:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 994, 1: 1042, 2: 965, 3: 997, 4: 990, 5: 1029, 6: 978, 7: 1015, 8: 961, 9: 1029}\n",
      "First 20 Labels: [8, 5, 0, 6, 9, 2, 8, 3, 6, 2, 7, 4, 6, 9, 0, 0, 7, 3, 7, 2]\n",
      "\n",
      "Example of Image 100:\n",
      "Image - Min Value: 4 Max Value: 255\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 8 Name: ship\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21ed66e1ba8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 3\n",
    "sample_id = 100\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    a, b = 0.0, 1.0\n",
    "    xmin, xmax = 0, 255\n",
    "    \n",
    "    return a + ((x-xmin) * (b-a))/(xmax-xmin)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    result = np.zeros((len(x), 10))\n",
    "    for i in range(len(x)):\n",
    "        result[i, x[i]] = 1\n",
    "        \n",
    "    return result \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a bach of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape=[None, image_shape[0], image_shape[1], image_shape[2]], name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape=(None, n_classes), name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    # Get the input and kernel dimensions\n",
    "    input_height, input_width, input_depth = list(map(int, x_tensor.get_shape().as_list()[1:]))\n",
    "    kernel_height, kernel_width = conv_ksize\n",
    "    \n",
    "    # Calculate the filter dimensions\n",
    "    filter_height = int((input_height - kernel_height + 2) / conv_strides[0] + 1)\n",
    "    filter_width = int((input_width - kernel_width + 2) / conv_strides[1] + 1)\n",
    "    \n",
    "    # Define the filter weights and the bias based on those dimensions\n",
    "    filter_weights = tf.Variable(tf.truncated_normal((filter_height, filter_width, input_depth, conv_num_outputs),\n",
    "                                                     mean=0.0,\n",
    "                                                     stddev=0.1))\n",
    "    filter_bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    \n",
    "    # Define the CNN\n",
    "    conv_layer = tf.nn.conv2d(x_tensor, \n",
    "                              filter=filter_weights, \n",
    "                              strides=[1, conv_strides[0], conv_strides[1], 1], \n",
    "                              padding='SAME')\n",
    "    conv_layer = tf.nn.bias_add(conv_layer, filter_bias)\n",
    "    \n",
    "    # Add a nonlinear activation\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    \n",
    "    # Add a pooling layer\n",
    "    conv_layer = tf.nn.max_pool(conv_layer,\n",
    "                                ksize=[1, pool_ksize[0], pool_ksize[1], 1],\n",
    "                                strides=[1, pool_strides[0], pool_strides[1], 1],\n",
    "                                padding='SAME')\n",
    "    \n",
    "    return conv_layer \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    height, width, depth = x_tensor.get_shape().as_list()[1:]\n",
    "    flatten_size = int(height * width * depth)\n",
    "    return tf.reshape(x_tensor, [-1, flatten_size])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function    \n",
    "    weights = tf.Variable(tf.truncated_normal([int(x_tensor.get_shape().as_list()[1]), num_outputs],\n",
    "                                              mean=0.0,\n",
    "                                              stddev=0.1))\n",
    "    bias = tf.Variable(tf.truncated_normal([num_outputs],\n",
    "                      mean=0.0,\n",
    "                      stddev=0.1))\n",
    "    \n",
    "    layer = tf.matmul(x_tensor, weights) + bias\n",
    "    \n",
    "    return layer\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return fully_conn(x_tensor, num_outputs)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv_layer = conv2d_maxpool(x, \n",
    "                                conv_num_outputs=20, \n",
    "                                conv_ksize=(16,16), \n",
    "                                conv_strides=(1,1), \n",
    "                                pool_ksize=(8,8), \n",
    "                                pool_strides=(1,1))\n",
    "    conv_layer = tf.nn.dropout(conv_layer, keep_prob)\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    flat_layer = flatten(conv_layer)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    fc_layer = fully_conn(flat_layer, 16)\n",
    "    fc_layer = tf.nn.relu(fc_layer)\n",
    "    fc_layer = tf.nn.dropout(fc_layer, keep_prob)\n",
    "    \n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    out_layer = output(fc_layer, 10)\n",
    "    out_layer = tf.nn.softmax(out_layer)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return out_layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer, feed_dict={\n",
    "        x: feature_batch,\n",
    "        y: label_batch,\n",
    "        keep_prob: keep_probability\n",
    "    })\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    loss = sess.run(cost, feed_dict={\n",
    "        x: feature_batch,\n",
    "        y: label_batch,\n",
    "        keep_prob: 1.\n",
    "    })\n",
    "    \n",
    "    valid_acc = sess.run(accuracy, feed_dict={\n",
    "        x: feature_batch,\n",
    "        y: label_batch,\n",
    "        keep_prob: 1.\n",
    "    })\n",
    "    \n",
    "    print('Loss: {} Validation Accuracy: {}'.format(loss, valid_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 1024\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss: 2.3005495071411133 Validation Accuracy: 0.125\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss: 2.248082160949707 Validation Accuracy: 0.1967821717262268\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss: 2.2268426418304443 Validation Accuracy: 0.21905940771102905\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss: 2.2104687690734863 Validation Accuracy: 0.23886138200759888\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss: 2.211364269256592 Validation Accuracy: 0.24381189048290253\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss: 2.18937087059021 Validation Accuracy: 0.2512376308441162\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss: 2.1877689361572266 Validation Accuracy: 0.2673267424106598\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss: 2.1615779399871826 Validation Accuracy: 0.2747524678707123\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss: 2.1545798778533936 Validation Accuracy: 0.2957920432090759\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss: 2.1574602127075195 Validation Accuracy: 0.30074256658554077\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss: 2.1452412605285645 Validation Accuracy: 0.2957920730113983\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss: 2.1457862854003906 Validation Accuracy: 0.30816832184791565\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss: 2.1468896865844727 Validation Accuracy: 0.3118811845779419\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss: 2.1327781677246094 Validation Accuracy: 0.3539603650569916\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss: 2.122872829437256 Validation Accuracy: 0.33539605140686035\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss: 2.1303505897521973 Validation Accuracy: 0.341584175825119\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss: 2.0871968269348145 Validation Accuracy: 0.375\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss: 2.0997893810272217 Validation Accuracy: 0.3774752616882324\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss: 2.0769033432006836 Validation Accuracy: 0.38861384987831116\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss: 2.0837042331695557 Validation Accuracy: 0.40594059228897095\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss: 2.0716605186462402 Validation Accuracy: 0.4047030210494995\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss: 2.0669634342193604 Validation Accuracy: 0.4183168113231659\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss: 2.0449366569519043 Validation Accuracy: 0.4183168411254883\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss: 2.045114278793335 Validation Accuracy: 0.4183168411254883\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss: 2.0592129230499268 Validation Accuracy: 0.41460394859313965\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss: 2.0526392459869385 Validation Accuracy: 0.4195544719696045\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss: 2.0425527095794678 Validation Accuracy: 0.4269801676273346\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss: 2.039945363998413 Validation Accuracy: 0.42450493574142456\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss: 2.0274429321289062 Validation Accuracy: 0.445544570684433\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss: 2.022737979888916 Validation Accuracy: 0.4443069398403168\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss: 2.018899440765381 Validation Accuracy: 0.45049506425857544\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss: 2.0103163719177246 Validation Accuracy: 0.46039602160453796\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss: 2.020744800567627 Validation Accuracy: 0.4344059228897095\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss: 2.0109260082244873 Validation Accuracy: 0.45792078971862793\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss: 2.0006375312805176 Validation Accuracy: 0.47648513317108154\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss: 2.0008740425109863 Validation Accuracy: 0.4628712832927704\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss: 1.9998664855957031 Validation Accuracy: 0.4702969789505005\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss: 1.995540738105774 Validation Accuracy: 0.4826732873916626\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss: 1.9909493923187256 Validation Accuracy: 0.4913366138935089\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss: 2.0093326568603516 Validation Accuracy: 0.46905940771102905\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss: 2.0034356117248535 Validation Accuracy: 0.4641088843345642\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss: 2.001758098602295 Validation Accuracy: 0.45792078971862793\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss: 1.9963555335998535 Validation Accuracy: 0.4826732277870178\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss: 1.9896472692489624 Validation Accuracy: 0.4826732575893402\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss: 1.9892780780792236 Validation Accuracy: 0.48391085863113403\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss: 1.9760679006576538 Validation Accuracy: 0.5037128329277039\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss: 1.979256510734558 Validation Accuracy: 0.49381187558174133\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss: 1.9861046075820923 Validation Accuracy: 0.4913366138935089\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss: 1.9745714664459229 Validation Accuracy: 0.4975247383117676\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss: 1.9837197065353394 Validation Accuracy: 0.49628713726997375\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss: 1.9655694961547852 Validation Accuracy: 0.5210395455360413\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss: 1.963686227798462 Validation Accuracy: 0.5210395455360413\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss: 1.9752355813980103 Validation Accuracy: 0.5111385583877563\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss: 1.972994089126587 Validation Accuracy: 0.5012376308441162\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss: 1.9604532718658447 Validation Accuracy: 0.5136138796806335\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss: 1.967075228691101 Validation Accuracy: 0.5099009871482849\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss: 1.988046407699585 Validation Accuracy: 0.4801979959011078\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss: 1.9725303649902344 Validation Accuracy: 0.5136138796806335\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss: 1.9706008434295654 Validation Accuracy: 0.5160890817642212\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss: 1.9567853212356567 Validation Accuracy: 0.5198019742965698\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss: 1.9567878246307373 Validation Accuracy: 0.5222771763801575\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss: 1.9657161235809326 Validation Accuracy: 0.5037128329277039\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss: 1.960770606994629 Validation Accuracy: 0.5198019742965698\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss: 1.9553625583648682 Validation Accuracy: 0.5173267126083374\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss: 1.964963436126709 Validation Accuracy: 0.5024752020835876\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss: 1.947472095489502 Validation Accuracy: 0.5235148072242737\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss: 1.9589829444885254 Validation Accuracy: 0.5272276997566223\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss: 1.9532008171081543 Validation Accuracy: 0.5297029614448547\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss: 1.9424594640731812 Validation Accuracy: 0.5457920432090759\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss: 1.957260012626648 Validation Accuracy: 0.5259900689125061\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss: 1.9314548969268799 Validation Accuracy: 0.5445544719696045\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss: 1.9533421993255615 Validation Accuracy: 0.5198019742965698\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss: 1.944826602935791 Validation Accuracy: 0.5321781635284424\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss: 1.941446304321289 Validation Accuracy: 0.5297029614448547\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss: 1.9391363859176636 Validation Accuracy: 0.5396039485931396\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss: 1.954541802406311 Validation Accuracy: 0.5272276997566223\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss: 1.9376893043518066 Validation Accuracy: 0.5457920432090759\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss: 1.928943157196045 Validation Accuracy: 0.5532178282737732\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss: 1.929639458656311 Validation Accuracy: 0.5420792102813721\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss: 1.9576460123062134 Validation Accuracy: 0.5210395455360413\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss: 1.9309537410736084 Validation Accuracy: 0.5470297336578369\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss: 1.9309810400009155 Validation Accuracy: 0.5445544719696045\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss: 1.9417937994003296 Validation Accuracy: 0.5495049357414246\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss: 1.9289023876190186 Validation Accuracy: 0.5532178282737732\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss: 1.9418978691101074 Validation Accuracy: 0.5247524380683899\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss: 1.940075159072876 Validation Accuracy: 0.5284653306007385\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss: 1.9311330318450928 Validation Accuracy: 0.5383663177490234\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss: 1.9275145530700684 Validation Accuracy: 0.5495048761367798\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss: 1.9388974905014038 Validation Accuracy: 0.5247524380683899\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss: 1.9286274909973145 Validation Accuracy: 0.5532178282737732\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss: 1.9300578832626343 Validation Accuracy: 0.5445544719696045\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss: 1.9332945346832275 Validation Accuracy: 0.5445544719696045\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss: 1.924678087234497 Validation Accuracy: 0.5618811845779419\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss: 1.9357013702392578 Validation Accuracy: 0.5433167815208435\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss: 1.9249107837677002 Validation Accuracy: 0.5495049357414246\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss: 1.9323716163635254 Validation Accuracy: 0.5445544123649597\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss: 1.9366072416305542 Validation Accuracy: 0.535891056060791\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss: 1.9319820404052734 Validation Accuracy: 0.5321781635284424\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss: 1.9172451496124268 Validation Accuracy: 0.5594059228897095\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss: 1.9274524450302124 Validation Accuracy: 0.5482673048973083\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss: 2.3012521266937256 Validation Accuracy: 0.1435643583536148\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss: 2.300428628921509 Validation Accuracy: 0.12871287763118744\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss: 2.2695584297180176 Validation Accuracy: 0.16460394859313965\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss: 2.2543110847473145 Validation Accuracy: 0.22896039485931396\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss: 2.2378640174865723 Validation Accuracy: 0.23886138200759888\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss: 2.233008623123169 Validation Accuracy: 0.2326732575893402\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss: 2.219986915588379 Validation Accuracy: 0.23886136710643768\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss: 2.2188777923583984 Validation Accuracy: 0.22524751722812653\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss: 2.1962127685546875 Validation Accuracy: 0.2512376308441162\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss: 2.199801206588745 Validation Accuracy: 0.27351483702659607\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss: 2.1864044666290283 Validation Accuracy: 0.26237624883651733\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss: 2.162062406539917 Validation Accuracy: 0.29950493574142456\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss: 2.150686740875244 Validation Accuracy: 0.3180692791938782\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss: 2.1427001953125 Validation Accuracy: 0.32797032594680786\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss: 2.1797120571136475 Validation Accuracy: 0.2920791804790497\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss: 2.138028383255005 Validation Accuracy: 0.3205445110797882\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss: 2.1407058238983154 Validation Accuracy: 0.32673266530036926\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss: 2.1267025470733643 Validation Accuracy: 0.3403465151786804\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss: 2.1295135021209717 Validation Accuracy: 0.3329208195209503\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss: 2.1441681385040283 Validation Accuracy: 0.3304455280303955\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss: 2.1123955249786377 Validation Accuracy: 0.3452970087528229\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss: 2.1240720748901367 Validation Accuracy: 0.33415842056274414\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss: 2.0876200199127197 Validation Accuracy: 0.3836633265018463\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss: 2.1024625301361084 Validation Accuracy: 0.36386141180992126\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss: 2.121715545654297 Validation Accuracy: 0.3465346395969391\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss: 2.098752975463867 Validation Accuracy: 0.37004947662353516\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss: 2.0931224822998047 Validation Accuracy: 0.37004950642585754\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss: 2.0899922847747803 Validation Accuracy: 0.37747520208358765\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss: 2.1198763847351074 Validation Accuracy: 0.3378712832927704\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss: 2.080314874649048 Validation Accuracy: 0.38985151052474976\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss: 2.0840108394622803 Validation Accuracy: 0.37128710746765137\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss: 2.073945999145508 Validation Accuracy: 0.3948019742965698\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss: 2.0533885955810547 Validation Accuracy: 0.40594059228897095\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss: 2.0596516132354736 Validation Accuracy: 0.4183168411254883\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss: 2.0939981937408447 Validation Accuracy: 0.3589108884334564\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss: 2.0641660690307617 Validation Accuracy: 0.38861387968063354\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss: 2.054349899291992 Validation Accuracy: 0.4183168411254883\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss: 2.052525281906128 Validation Accuracy: 0.41460397839546204\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss: 2.033254861831665 Validation Accuracy: 0.44183164834976196\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss: 2.0657811164855957 Validation Accuracy: 0.3910891115665436\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss: 2.0325536727905273 Validation Accuracy: 0.43316832184791565\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss: 2.0329489707946777 Validation Accuracy: 0.4282178282737732\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss: 2.0327184200286865 Validation Accuracy: 0.44059404730796814\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss: 2.022679090499878 Validation Accuracy: 0.4443069100379944\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss: 2.053584575653076 Validation Accuracy: 0.40594059228897095\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss: 2.019322156906128 Validation Accuracy: 0.4467821717262268\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss: 2.0400092601776123 Validation Accuracy: 0.4207920730113983\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss: 2.015413522720337 Validation Accuracy: 0.45792078971862793\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss: 2.0196001529693604 Validation Accuracy: 0.4529702961444855\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss: 2.0519726276397705 Validation Accuracy: 0.40594059228897095\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss: 2.0250816345214844 Validation Accuracy: 0.43193066120147705\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss: 2.0307860374450684 Validation Accuracy: 0.42450496554374695\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss: 2.0149130821228027 Validation Accuracy: 0.4554455280303955\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss: 2.0194056034088135 Validation Accuracy: 0.4443069398403168\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss: 2.0312488079071045 Validation Accuracy: 0.42574259638786316\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss: 2.019068717956543 Validation Accuracy: 0.44059404730796814\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss: 2.0111265182495117 Validation Accuracy: 0.45173266530036926\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss: 1.9983723163604736 Validation Accuracy: 0.4653465151786804\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss: 1.9870723485946655 Validation Accuracy: 0.4801979959011078\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss: 2.024028778076172 Validation Accuracy: 0.44183170795440674\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss: 2.011233329772949 Validation Accuracy: 0.4554455280303955\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss: 2.0012123584747314 Validation Accuracy: 0.47029703855514526\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss: 1.9877541065216064 Validation Accuracy: 0.4789603650569916\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss: 1.9867451190948486 Validation Accuracy: 0.47648513317108154\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss: 2.018338680267334 Validation Accuracy: 0.45668312907218933\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss: 2.00282621383667 Validation Accuracy: 0.466584175825119\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss: 1.999079942703247 Validation Accuracy: 0.4616336524486542\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss: 2.003207206726074 Validation Accuracy: 0.46782174706459045\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss: 2.001646041870117 Validation Accuracy: 0.4653465151786804\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss: 2.040990114212036 Validation Accuracy: 0.4245049059391022\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss: 2.017366409301758 Validation Accuracy: 0.4480198323726654\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss: 1.995748519897461 Validation Accuracy: 0.4653465449810028\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss: 1.9887744188308716 Validation Accuracy: 0.4814356565475464\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss: 1.9822429418563843 Validation Accuracy: 0.47648513317108154\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss: 2.010599374771118 Validation Accuracy: 0.44306930899620056\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss: 1.9931387901306152 Validation Accuracy: 0.4801979959011078\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss: 2.0062501430511475 Validation Accuracy: 0.44925743341445923\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss: 1.9866104125976562 Validation Accuracy: 0.4913366138935089\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss: 1.9734954833984375 Validation Accuracy: 0.49504953622817993\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss: 2.0025253295898438 Validation Accuracy: 0.44801977276802063\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss: 1.9833934307098389 Validation Accuracy: 0.48638612031936646\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss: 2.0013787746429443 Validation Accuracy: 0.4616336226463318\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss: 2.001919984817505 Validation Accuracy: 0.46905937790870667\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss: 1.9764244556427002 Validation Accuracy: 0.481435626745224\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss: 2.0215582847595215 Validation Accuracy: 0.4356435537338257\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss: 1.9837608337402344 Validation Accuracy: 0.4789603352546692\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss: 1.9942638874053955 Validation Accuracy: 0.4715346395969391\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss: 1.9741337299346924 Validation Accuracy: 0.4925742745399475\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss: 1.9726157188415527 Validation Accuracy: 0.4900989532470703\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss: 2.0343894958496094 Validation Accuracy: 0.4170791804790497\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss: 1.9785375595092773 Validation Accuracy: 0.48762375116348267\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss: 1.9975861310958862 Validation Accuracy: 0.47524750232696533\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss: 2.0211260318756104 Validation Accuracy: 0.4455445408821106\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss: 1.9757201671600342 Validation Accuracy: 0.49504947662353516\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss: 2.014191150665283 Validation Accuracy: 0.4455445408821106\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss: 1.993837594985962 Validation Accuracy: 0.4740098714828491\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss: 1.9876314401626587 Validation Accuracy: 0.4715346395969391\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss: 1.9827945232391357 Validation Accuracy: 0.4913366138935089\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss: 1.9790228605270386 Validation Accuracy: 0.4888613820075989\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss: 2.002375841140747 Validation Accuracy: 0.47648513317108154\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss: 1.9685556888580322 Validation Accuracy: 0.496287077665329\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss: 1.9767698049545288 Validation Accuracy: 0.48514848947525024\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss: 1.9841457605361938 Validation Accuracy: 0.47772276401519775\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss: 1.9803988933563232 Validation Accuracy: 0.4826732575893402\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss: 2.006074905395508 Validation Accuracy: 0.4566831588745117\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss: 1.9797430038452148 Validation Accuracy: 0.4752475321292877\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss: 1.997011661529541 Validation Accuracy: 0.45049503445625305\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss: 1.981445074081421 Validation Accuracy: 0.4801979660987854\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss: 1.9803576469421387 Validation Accuracy: 0.48762375116348267\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss: 2.0046815872192383 Validation Accuracy: 0.4542078971862793\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss: 1.983948826789856 Validation Accuracy: 0.4801980257034302\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss: 1.9873276948928833 Validation Accuracy: 0.47648513317108154\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss: 1.967283010482788 Validation Accuracy: 0.514851450920105\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss: 1.9650171995162964 Validation Accuracy: 0.48762378096580505\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss: 1.9740544557571411 Validation Accuracy: 0.47896039485931396\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss: 1.9668900966644287 Validation Accuracy: 0.5111385583877563\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss: 1.9681426286697388 Validation Accuracy: 0.4913366138935089\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss: 1.979809045791626 Validation Accuracy: 0.48391085863113403\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss: 1.971752643585205 Validation Accuracy: 0.48762375116348267\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss: 1.9795143604278564 Validation Accuracy: 0.4888613522052765\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss: 1.9622081518173218 Validation Accuracy: 0.5185643434524536\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss: 1.9850481748580933 Validation Accuracy: 0.4789603650569916\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss: 1.9564062356948853 Validation Accuracy: 0.5037128925323486\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss: 1.966071605682373 Validation Accuracy: 0.49752479791641235\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss: 1.965395450592041 Validation Accuracy: 0.49628713726997375\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss: 1.978464126586914 Validation Accuracy: 0.48391085863113403\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss: 1.9882874488830566 Validation Accuracy: 0.4715346693992615\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss: 1.9711034297943115 Validation Accuracy: 0.49504947662353516\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss: 1.9606266021728516 Validation Accuracy: 0.49628710746765137\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss: 1.9659233093261719 Validation Accuracy: 0.4913366138935089\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss: 1.9536986351013184 Validation Accuracy: 0.5074257254600525\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss: 1.9716137647628784 Validation Accuracy: 0.4913366436958313\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss: 1.9745376110076904 Validation Accuracy: 0.49381187558174133\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss: 1.9694182872772217 Validation Accuracy: 0.4900989830493927\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss: 1.96903657913208 Validation Accuracy: 0.4863860607147217\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss: 1.9698251485824585 Validation Accuracy: 0.496287077665329\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss: 1.96624755859375 Validation Accuracy: 0.48638615012168884\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss: 1.9608981609344482 Validation Accuracy: 0.5185643434524536\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss: 1.9591197967529297 Validation Accuracy: 0.5086633563041687\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss: 1.9778075218200684 Validation Accuracy: 0.47896039485931396\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss: 1.9495007991790771 Validation Accuracy: 0.5297029614448547\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss: 1.9732909202575684 Validation Accuracy: 0.4900989830493927\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss: 1.9687762260437012 Validation Accuracy: 0.5012376308441162\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss: 1.9586912393569946 Validation Accuracy: 0.5074257254600525\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss: 1.9725399017333984 Validation Accuracy: 0.48638615012168884\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss: 1.9586687088012695 Validation Accuracy: 0.5074257254600525\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss: 1.9486968517303467 Validation Accuracy: 0.5160890817642212\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss: 1.971442461013794 Validation Accuracy: 0.49628713726997375\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss: 1.944441556930542 Validation Accuracy: 0.514851450920105\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss: 1.9707715511322021 Validation Accuracy: 0.4938119053840637\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss: 1.9465163946151733 Validation Accuracy: 0.5235148668289185\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss: 1.9532414674758911 Validation Accuracy: 0.5086632966995239\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss: 1.9623034000396729 Validation Accuracy: 0.5123762488365173\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss: 1.9393903017044067 Validation Accuracy: 0.5346534252166748\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss: 1.9507368803024292 Validation Accuracy: 0.5284653306007385\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss: 1.9536678791046143 Validation Accuracy: 0.5099009871482849\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss: 1.9630053043365479 Validation Accuracy: 0.49381187558174133\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss: 1.9496188163757324 Validation Accuracy: 0.5061880946159363\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss: 1.9379570484161377 Validation Accuracy: 0.5321781635284424\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss: 1.9604666233062744 Validation Accuracy: 0.5012375712394714\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss: 1.958518385887146 Validation Accuracy: 0.49628710746765137\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss: 1.9521896839141846 Validation Accuracy: 0.5099009871482849\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss: 1.971925139427185 Validation Accuracy: 0.4925742745399475\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss: 1.9401077032089233 Validation Accuracy: 0.5235148072242737\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss: 1.966956377029419 Validation Accuracy: 0.5012375712394714\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss: 1.9544684886932373 Validation Accuracy: 0.5024752020835876\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss: 1.9458767175674438 Validation Accuracy: 0.5198019742965698\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss: 1.9417052268981934 Validation Accuracy: 0.5222772359848022\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss: 1.9368302822113037 Validation Accuracy: 0.5334157943725586\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss: 1.9706809520721436 Validation Accuracy: 0.4900989830493927\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss: 1.957586407661438 Validation Accuracy: 0.4987623393535614\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss: 1.9383339881896973 Validation Accuracy: 0.5235147476196289\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss: 1.952041506767273 Validation Accuracy: 0.5222771763801575\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss: 1.9426368474960327 Validation Accuracy: 0.5222771763801575\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss: 1.965153455734253 Validation Accuracy: 0.4975247383117676\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss: 1.9419978857040405 Validation Accuracy: 0.5185643434524536\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss: 1.9444336891174316 Validation Accuracy: 0.5259900689125061\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss: 1.949693202972412 Validation Accuracy: 0.514851450920105\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss: 1.9340555667877197 Validation Accuracy: 0.535891056060791\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss: 1.9740952253341675 Validation Accuracy: 0.48267319798469543\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss: 1.9462883472442627 Validation Accuracy: 0.519801914691925\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss: 1.9398703575134277 Validation Accuracy: 0.521039605140686\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss: 1.9490759372711182 Validation Accuracy: 0.5086633563041687\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss: 1.9219160079956055 Validation Accuracy: 0.5507426261901855\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss: 1.9599789381027222 Validation Accuracy: 0.49257418513298035\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss: 1.9376016855239868 Validation Accuracy: 0.516089141368866\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss: 1.9505709409713745 Validation Accuracy: 0.5049504637718201\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss: 1.9499229192733765 Validation Accuracy: 0.5284653306007385\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss: 1.9532548189163208 Validation Accuracy: 0.5099009871482849\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss: 1.9467859268188477 Validation Accuracy: 0.5123761892318726\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss: 1.934570550918579 Validation Accuracy: 0.5272277593612671\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss: 1.9493180513381958 Validation Accuracy: 0.5222771763801575\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss: 1.9639599323272705 Validation Accuracy: 0.5012376308441162\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss: 1.9279024600982666 Validation Accuracy: 0.5445544123649597\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss: 1.9477643966674805 Validation Accuracy: 0.514851450920105\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss: 1.9533931016921997 Validation Accuracy: 0.5123762488365173\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss: 1.9415857791900635 Validation Accuracy: 0.5297029614448547\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss: 1.948263168334961 Validation Accuracy: 0.5136138200759888\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss: 1.93402099609375 Validation Accuracy: 0.530940592288971\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss: 1.9688385725021362 Validation Accuracy: 0.4900990128517151\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss: 1.9428832530975342 Validation Accuracy: 0.5222772359848022\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss: 1.9401037693023682 Validation Accuracy: 0.5309405326843262\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss: 1.9615644216537476 Validation Accuracy: 0.5024752616882324\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss: 1.9438523054122925 Validation Accuracy: 0.5123761892318726\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss: 1.9559768438339233 Validation Accuracy: 0.5148515105247498\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss: 1.9370423555374146 Validation Accuracy: 0.5222772359848022\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss: 1.9569512605667114 Validation Accuracy: 0.5074257254600525\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss: 1.9408785104751587 Validation Accuracy: 0.5247524380683899\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss: 1.9393742084503174 Validation Accuracy: 0.535891056060791\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss: 1.9470762014389038 Validation Accuracy: 0.521039605140686\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss: 1.9414098262786865 Validation Accuracy: 0.5247524380683899\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss: 1.945686936378479 Validation Accuracy: 0.521039605140686\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss: 1.9348931312561035 Validation Accuracy: 0.5334158539772034\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss: 1.9341917037963867 Validation Accuracy: 0.5334157943725586\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss: 1.9714725017547607 Validation Accuracy: 0.4814355969429016\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss: 1.9370990991592407 Validation Accuracy: 0.521039605140686\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss: 1.937548041343689 Validation Accuracy: 0.5160890817642212\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss: 1.9556524753570557 Validation Accuracy: 0.5074257254600525\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss: 1.957890510559082 Validation Accuracy: 0.5049505233764648\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss: 1.9382891654968262 Validation Accuracy: 0.5297029614448547\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss: 1.9224878549575806 Validation Accuracy: 0.5371286869049072\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss: 1.930923581123352 Validation Accuracy: 0.5371286869049072\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss: 1.9541691541671753 Validation Accuracy: 0.514851450920105\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss: 1.917528510093689 Validation Accuracy: 0.556930661201477\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss: 1.9294220209121704 Validation Accuracy: 0.530940592288971\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss: 1.93110990524292 Validation Accuracy: 0.5334157943725586\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss: 1.9527192115783691 Validation Accuracy: 0.5111385583877563\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss: 1.9432886838912964 Validation Accuracy: 0.5222772359848022\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss: 1.9248697757720947 Validation Accuracy: 0.5383663177490234\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss: 1.925475835800171 Validation Accuracy: 0.5445544719696045\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss: 1.9483084678649902 Validation Accuracy: 0.5099010467529297\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss: 1.9314345121383667 Validation Accuracy: 0.5408415794372559\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss: 1.9395577907562256 Validation Accuracy: 0.5297029614448547\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss: 1.9287235736846924 Validation Accuracy: 0.5321781635284424\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss: 1.9275503158569336 Validation Accuracy: 0.530940592288971\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss: 1.9379528760910034 Validation Accuracy: 0.5297029614448547\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss: 1.9232381582260132 Validation Accuracy: 0.5396039485931396\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss: 1.9336012601852417 Validation Accuracy: 0.535891056060791\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss: 1.9228276014328003 Validation Accuracy: 0.535891056060791\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss: 1.934808611869812 Validation Accuracy: 0.5272276997566223\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss: 1.9267605543136597 Validation Accuracy: 0.530940592288971\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss: 1.9247026443481445 Validation Accuracy: 0.5420792102813721\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss: 1.931654691696167 Validation Accuracy: 0.5321781635284424\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss: 1.9227850437164307 Validation Accuracy: 0.5383663177490234\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss: 1.9404321908950806 Validation Accuracy: 0.5185643434524536\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss: 1.9279942512512207 Validation Accuracy: 0.5297029614448547\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss: 1.912017822265625 Validation Accuracy: 0.5606435537338257\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss: 1.944909691810608 Validation Accuracy: 0.5136138200759888\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss: 1.9140334129333496 Validation Accuracy: 0.556930661201477\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss: 1.9351472854614258 Validation Accuracy: 0.5346534252166748\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss: 1.9482831954956055 Validation Accuracy: 0.516089141368866\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss: 1.9225690364837646 Validation Accuracy: 0.5383663177490234\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss: 1.938657522201538 Validation Accuracy: 0.5160890817642212\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss: 1.9200243949890137 Validation Accuracy: 0.5433168411254883\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss: 1.9434025287628174 Validation Accuracy: 0.5235148668289185\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss: 1.9360854625701904 Validation Accuracy: 0.5346534252166748\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss: 1.9375348091125488 Validation Accuracy: 0.5235148668289185\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss: 1.9222075939178467 Validation Accuracy: 0.5396038889884949\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss: 1.923341989517212 Validation Accuracy: 0.5445544123649597\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss: 1.9188796281814575 Validation Accuracy: 0.5470297336578369\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss: 1.9269543886184692 Validation Accuracy: 0.530940592288971\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss: 1.923525094985962 Validation Accuracy: 0.5470296740531921\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss: 1.9317107200622559 Validation Accuracy: 0.5346534252166748\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss: 1.9122836589813232 Validation Accuracy: 0.5420791506767273\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss: 1.9348039627075195 Validation Accuracy: 0.5284652709960938\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss: 1.9317947626113892 Validation Accuracy: 0.521039605140686\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss: 1.9262046813964844 Validation Accuracy: 0.5297029614448547\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss: 1.9199364185333252 Validation Accuracy: 0.550742506980896\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss: 1.9181238412857056 Validation Accuracy: 0.5383663177490234\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss: 1.924896240234375 Validation Accuracy: 0.5346534252166748\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss: 1.9450055360794067 Validation Accuracy: 0.5136138200759888\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss: 1.9212276935577393 Validation Accuracy: 0.5433168411254883\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss: 1.9300330877304077 Validation Accuracy: 0.5470297336578369\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss: 1.9133106470108032 Validation Accuracy: 0.5482673048973083\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss: 1.9242513179779053 Validation Accuracy: 0.5346534252166748\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss: 1.9290636777877808 Validation Accuracy: 0.5284653306007385\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss: 1.9193542003631592 Validation Accuracy: 0.5495049357414246\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss: 1.9304475784301758 Validation Accuracy: 0.5259900689125061\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss: 1.9205420017242432 Validation Accuracy: 0.5396039485931396\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss: 1.9083590507507324 Validation Accuracy: 0.5519801378250122\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss: 1.9292731285095215 Validation Accuracy: 0.5309405326843262\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss: 1.9222040176391602 Validation Accuracy: 0.5507425665855408\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss: 1.9343560934066772 Validation Accuracy: 0.52970290184021\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss: 1.9105379581451416 Validation Accuracy: 0.5556930303573608\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss: 1.9232177734375 Validation Accuracy: 0.5346534252166748\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss: 1.9452310800552368 Validation Accuracy: 0.5111386179924011\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss: 1.9101461172103882 Validation Accuracy: 0.556930661201477\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss: 1.9271342754364014 Validation Accuracy: 0.5408415794372559\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss: 1.9396320581436157 Validation Accuracy: 0.5173267126083374\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss: 1.9225142002105713 Validation Accuracy: 0.5334157943725586\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss: 1.9275710582733154 Validation Accuracy: 0.5334157943725586\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss: 1.9210528135299683 Validation Accuracy: 0.5346534252166748\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss: 1.9164133071899414 Validation Accuracy: 0.5420792102813721\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss: 1.9188066720962524 Validation Accuracy: 0.5433167815208435\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss: 1.922273874282837 Validation Accuracy: 0.5383663177490234\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss: 1.9343082904815674 Validation Accuracy: 0.5185643434524536\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss: 1.917520523071289 Validation Accuracy: 0.5445544719696045\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss: 1.9222614765167236 Validation Accuracy: 0.5470296740531921\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss: 1.9007450342178345 Validation Accuracy: 0.566831648349762\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss: 1.9178763628005981 Validation Accuracy: 0.5433167815208435\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss: 1.9483423233032227 Validation Accuracy: 0.5012375712394714\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss: 1.9083747863769531 Validation Accuracy: 0.5643564462661743\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss: 1.9142643213272095 Validation Accuracy: 0.5507426261901855\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss: 1.9167131185531616 Validation Accuracy: 0.5470296740531921\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss: 1.9220082759857178 Validation Accuracy: 0.5396038889884949\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss: 1.9544697999954224 Validation Accuracy: 0.5024752616882324\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss: 1.9067389965057373 Validation Accuracy: 0.5606435537338257\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss: 1.9122788906097412 Validation Accuracy: 0.5495049953460693\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss: 1.8986239433288574 Validation Accuracy: 0.5767326354980469\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss: 1.9245249032974243 Validation Accuracy: 0.535891056060791\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss: 1.913977861404419 Validation Accuracy: 0.5408415198326111\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss: 1.913895845413208 Validation Accuracy: 0.551980197429657\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss: 1.9157767295837402 Validation Accuracy: 0.5532177686691284\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss: 1.8936183452606201 Validation Accuracy: 0.573019802570343\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss: 1.916722059249878 Validation Accuracy: 0.5544554591178894\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss: 1.9462194442749023 Validation Accuracy: 0.5136138200759888\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss: 1.9173232316970825 Validation Accuracy: 0.5457920432090759\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss: 1.9250158071517944 Validation Accuracy: 0.5420792102813721\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss: 1.9111857414245605 Validation Accuracy: 0.5507425665855408\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss: 1.9231852293014526 Validation Accuracy: 0.550742506980896\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss: 1.936178207397461 Validation Accuracy: 0.516089141368866\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss: 1.8983875513076782 Validation Accuracy: 0.573019802570343\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss: 1.9193289279937744 Validation Accuracy: 0.5420791506767273\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss: 1.9009243249893188 Validation Accuracy: 0.5618811845779419\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss: 1.908448338508606 Validation Accuracy: 0.5495049357414246\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss: 1.9314024448394775 Validation Accuracy: 0.5272276997566223\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss: 1.9178544282913208 Validation Accuracy: 0.5470296740531921\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss: 1.9350718259811401 Validation Accuracy: 0.5334158539772034\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss: 1.8999155759811401 Validation Accuracy: 0.5705445408821106\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss: 1.911892056465149 Validation Accuracy: 0.5482673048973083\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss: 1.9175082445144653 Validation Accuracy: 0.5495049357414246\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss: 1.9116778373718262 Validation Accuracy: 0.5556930899620056\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss: 1.918527603149414 Validation Accuracy: 0.5594059228897095\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss: 1.9064823389053345 Validation Accuracy: 0.5643563866615295\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss: 1.9091441631317139 Validation Accuracy: 0.5495049357414246\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss: 1.9388923645019531 Validation Accuracy: 0.5222771763801575\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss: 1.942018985748291 Validation Accuracy: 0.5160890817642212\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss: 1.9242339134216309 Validation Accuracy: 0.5433167815208435\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss: 1.901827096939087 Validation Accuracy: 0.5631188154220581\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss: 1.9001432657241821 Validation Accuracy: 0.5693069100379944\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss: 1.9172321557998657 Validation Accuracy: 0.5457920432090759\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss: 1.9103056192398071 Validation Accuracy: 0.5532178282737732\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss: 1.900101661682129 Validation Accuracy: 0.5705446004867554\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss: 1.9035321474075317 Validation Accuracy: 0.5631188154220581\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss: 1.901507019996643 Validation Accuracy: 0.5618811845779419\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss: 1.9258344173431396 Validation Accuracy: 0.5346534252166748\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss: 1.9073083400726318 Validation Accuracy: 0.5569307208061218\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss: 1.9094423055648804 Validation Accuracy: 0.5680692791938782\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss: 1.9131598472595215 Validation Accuracy: 0.5482673048973083\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss: 1.9097368717193604 Validation Accuracy: 0.5532177686691284\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss: 1.9288960695266724 Validation Accuracy: 0.521039605140686\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss: 1.9144853353500366 Validation Accuracy: 0.5396038889884949\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss: 1.9106135368347168 Validation Accuracy: 0.5581682920455933\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss: 1.8900773525238037 Validation Accuracy: 0.5705445408821106\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss: 1.9115861654281616 Validation Accuracy: 0.5655940175056458\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss: 1.9232553243637085 Validation Accuracy: 0.5396039485931396\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss: 1.9045270681381226 Validation Accuracy: 0.5569307208061218\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss: 1.9113025665283203 Validation Accuracy: 0.5457920432090759\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss: 1.8841629028320312 Validation Accuracy: 0.5767326354980469\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss: 1.9187158346176147 Validation Accuracy: 0.5445544123649597\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss: 1.9201667308807373 Validation Accuracy: 0.5396039485931396\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss: 1.8917957544326782 Validation Accuracy: 0.5767326354980469\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss: 1.9051451683044434 Validation Accuracy: 0.5618811845779419\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss: 1.8994983434677124 Validation Accuracy: 0.5655940771102905\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss: 1.8940458297729492 Validation Accuracy: 0.5705445408821106\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss: 1.9323153495788574 Validation Accuracy: 0.5185643434524536\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss: 1.908942461013794 Validation Accuracy: 0.5556930303573608\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss: 1.925732135772705 Validation Accuracy: 0.5383663177490234\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss: 1.922877550125122 Validation Accuracy: 0.5445544123649597\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss: 1.9079567193984985 Validation Accuracy: 0.5606434941291809\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss: 1.902936577796936 Validation Accuracy: 0.5532177686691284\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss: 1.9026844501495361 Validation Accuracy: 0.5618811845779419\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss: 1.9330967664718628 Validation Accuracy: 0.52970290184021\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss: 1.8882719278335571 Validation Accuracy: 0.5816831588745117\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss: 1.9063094854354858 Validation Accuracy: 0.5594059228897095\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss: 1.8994590044021606 Validation Accuracy: 0.5594059228897095\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss: 1.8952324390411377 Validation Accuracy: 0.5742573738098145\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss: 1.935547947883606 Validation Accuracy: 0.530940592288971\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss: 1.8910244703292847 Validation Accuracy: 0.5829207897186279\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss: 1.9047213792800903 Validation Accuracy: 0.5680692791938782\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss: 1.91083824634552 Validation Accuracy: 0.5532177686691284\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss: 1.9046969413757324 Validation Accuracy: 0.5594059228897095\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss: 1.9165740013122559 Validation Accuracy: 0.5556930303573608\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss: 1.9063040018081665 Validation Accuracy: 0.551980197429657\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss: 1.8997670412063599 Validation Accuracy: 0.5680692791938782\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss: 1.9188605546951294 Validation Accuracy: 0.5445544123649597\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss: 1.89500093460083 Validation Accuracy: 0.5693069100379944\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss: 1.9117381572723389 Validation Accuracy: 0.5470297336578369\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss: 1.9138344526290894 Validation Accuracy: 0.5482673048973083\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss: 1.9080439805984497 Validation Accuracy: 0.5594059228897095\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss: 1.9047307968139648 Validation Accuracy: 0.556930661201477\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss: 1.914665937423706 Validation Accuracy: 0.5445544123649597\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss: 1.8932894468307495 Validation Accuracy: 0.573019802570343\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss: 1.8822662830352783 Validation Accuracy: 0.5804455280303955\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss: 1.9028126001358032 Validation Accuracy: 0.5544554591178894\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss: 1.9070435762405396 Validation Accuracy: 0.5618811845779419\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss: 1.8796229362487793 Validation Accuracy: 0.5915841460227966\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss: 1.8926668167114258 Validation Accuracy: 0.5680692791938782\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss: 1.8835633993148804 Validation Accuracy: 0.5792078971862793\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss: 1.9043266773223877 Validation Accuracy: 0.5631188154220581\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss: 1.913377285003662 Validation Accuracy: 0.5556930303573608\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss: 1.9067816734313965 Validation Accuracy: 0.5581682920455933\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss: 1.911826252937317 Validation Accuracy: 0.5507425665855408\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss: 1.8757975101470947 Validation Accuracy: 0.5903464555740356\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss: 1.9149971008300781 Validation Accuracy: 0.5532177686691284\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss: 1.9079753160476685 Validation Accuracy: 0.5544554591178894\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss: 1.9277271032333374 Validation Accuracy: 0.535891056060791\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss: 1.8839737176895142 Validation Accuracy: 0.5866336822509766\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss: 1.8792484998703003 Validation Accuracy: 0.5903464555740356\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss: 1.8827370405197144 Validation Accuracy: 0.5915840864181519\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss: 1.909395694732666 Validation Accuracy: 0.5532177686691284\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss: 1.9045476913452148 Validation Accuracy: 0.5618811845779419\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss: 1.9002392292022705 Validation Accuracy: 0.5742573738098145\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss: 1.8873403072357178 Validation Accuracy: 0.5730197429656982\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss: 1.9113291501998901 Validation Accuracy: 0.5457920432090759\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss: 1.9175939559936523 Validation Accuracy: 0.5383662581443787\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss: 1.9085984230041504 Validation Accuracy: 0.5482673048973083\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss: 1.8927150964736938 Validation Accuracy: 0.5754950642585754\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss: 1.8886340856552124 Validation Accuracy: 0.5742573738098145\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss: 1.9041800498962402 Validation Accuracy: 0.5556930303573608\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss: 1.9037904739379883 Validation Accuracy: 0.5594059228897095\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss: 1.8962154388427734 Validation Accuracy: 0.5643564462661743\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss: 1.8970363140106201 Validation Accuracy: 0.5779702663421631\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss: 1.8974733352661133 Validation Accuracy: 0.5618811249732971\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss: 1.8893566131591797 Validation Accuracy: 0.5779702663421631\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss: 1.9190202951431274 Validation Accuracy: 0.5420791506767273\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss: 1.8920708894729614 Validation Accuracy: 0.5655940175056458\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss: 1.895914912223816 Validation Accuracy: 0.5705444812774658\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss: 1.884700894355774 Validation Accuracy: 0.5866336822509766\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss: 1.8960148096084595 Validation Accuracy: 0.5693069696426392\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss: 1.9117431640625 Validation Accuracy: 0.5457920432090759\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss: 1.8936315774917603 Validation Accuracy: 0.5705444812774658\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss: 1.9012634754180908 Validation Accuracy: 0.5544553995132446\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss: 1.8941649198532104 Validation Accuracy: 0.5804455280303955\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss: 1.9137330055236816 Validation Accuracy: 0.5495049357414246\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss: 1.9011359214782715 Validation Accuracy: 0.5631188154220581\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss: 1.8806602954864502 Validation Accuracy: 0.5816831588745117\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss: 1.8994263410568237 Validation Accuracy: 0.5631188154220581\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss: 1.8873264789581299 Validation Accuracy: 0.5705444812774658\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss: 1.8870278596878052 Validation Accuracy: 0.5804455280303955\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss: 1.9055639505386353 Validation Accuracy: 0.5569306015968323\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss: 1.8869562149047852 Validation Accuracy: 0.5792078971862793\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss: 1.874497652053833 Validation Accuracy: 0.594059407711029\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss: 1.8894352912902832 Validation Accuracy: 0.5643564462661743\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss: 1.8960245847702026 Validation Accuracy: 0.5655940771102905\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss: 1.9084064960479736 Validation Accuracy: 0.5457920432090759\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss: 1.8936398029327393 Validation Accuracy: 0.5742573738098145\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss: 1.8783255815505981 Validation Accuracy: 0.5977722406387329\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss: 1.880639910697937 Validation Accuracy: 0.5965346097946167\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss: 1.878812551498413 Validation Accuracy: 0.5965346693992615\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss: 1.906592607498169 Validation Accuracy: 0.5556930303573608\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss: 1.9058058261871338 Validation Accuracy: 0.5594059228897095\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss: 1.904477834701538 Validation Accuracy: 0.5618811249732971\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss: 1.8775396347045898 Validation Accuracy: 0.5829207897186279\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss: 1.8939014673233032 Validation Accuracy: 0.5680692791938782\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss: 1.897961139678955 Validation Accuracy: 0.5618811845779419\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss: 1.881279468536377 Validation Accuracy: 0.5866336822509766\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss: 1.8850936889648438 Validation Accuracy: 0.5779702663421631\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss: 1.8766212463378906 Validation Accuracy: 0.5928217768669128\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss: 1.8972671031951904 Validation Accuracy: 0.5730197429656982\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss: 1.9040465354919434 Validation Accuracy: 0.550742506980896\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss: 1.905648946762085 Validation Accuracy: 0.5445544123649597\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss: 1.8909118175506592 Validation Accuracy: 0.5767326354980469\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss: 1.8899593353271484 Validation Accuracy: 0.573019802570343\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss: 1.8926663398742676 Validation Accuracy: 0.5730197429656982\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss: 1.9019994735717773 Validation Accuracy: 0.551980197429657\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss: 1.914054274559021 Validation Accuracy: 0.5556930303573608\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss: 1.9136759042739868 Validation Accuracy: 0.5544554591178894\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss: 1.8865201473236084 Validation Accuracy: 0.5680692791938782\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss: 1.9007092714309692 Validation Accuracy: 0.5606435537338257\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss: 1.90216064453125 Validation Accuracy: 0.5631187558174133\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss: 1.8915228843688965 Validation Accuracy: 0.568069338798523\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss: 1.9052605628967285 Validation Accuracy: 0.550742506980896\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss: 1.8650767803192139 Validation Accuracy: 0.6051979660987854\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss: 1.8758488893508911 Validation Accuracy: 0.5903465747833252\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss: 1.8932511806488037 Validation Accuracy: 0.5643563866615295\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss: 1.8865779638290405 Validation Accuracy: 0.573019802570343\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss: 1.8822598457336426 Validation Accuracy: 0.5779702663421631\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss: 1.8726625442504883 Validation Accuracy: 0.5853960514068604\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss: 1.901027798652649 Validation Accuracy: 0.5631188154220581\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss: 1.887805461883545 Validation Accuracy: 0.5705446004867554\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss: 1.906516432762146 Validation Accuracy: 0.5507425665855408\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss: 1.8855503797531128 Validation Accuracy: 0.5816831588745117\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss: 1.8793171644210815 Validation Accuracy: 0.5804454684257507\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss: 1.8875114917755127 Validation Accuracy: 0.5742573738098145\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss: 1.8980021476745605 Validation Accuracy: 0.5655940175056458\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss: 1.882517695426941 Validation Accuracy: 0.5866336226463318\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss: 1.900356650352478 Validation Accuracy: 0.5655940175056458\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss: 1.8826625347137451 Validation Accuracy: 0.5829207897186279\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss: 1.8843997716903687 Validation Accuracy: 0.5792078971862793\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss: 1.8939489126205444 Validation Accuracy: 0.5680692791938782\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss: 1.9082322120666504 Validation Accuracy: 0.5532177686691284\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss: 1.8933095932006836 Validation Accuracy: 0.568069338798523\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss: 1.874265432357788 Validation Accuracy: 0.5891088843345642\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss: 1.8915234804153442 Validation Accuracy: 0.5754950642585754\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss: 1.968404769897461 Validation Accuracy: 0.48514851927757263\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss: 1.921465277671814 Validation Accuracy: 0.5346534252166748\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss: 1.8929264545440674 Validation Accuracy: 0.5717821717262268\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss: 1.9026955366134644 Validation Accuracy: 0.556930661201477\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss: 1.9240405559539795 Validation Accuracy: 0.535891056060791\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.5005420923233033\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRESi/w8kc3DUF/lrVAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e73a845860>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-70% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 70%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
